{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "missing_files_list = []\n",
    "\n",
    "directory = \"fakenewsnet_dataset\\gossipcop\\\\fake\"\n",
    "for filename in os.listdir(directory):\n",
    "    file = os.path.join(directory, filename, \"news content.json\")\n",
    "    try:\n",
    "        with open(file, \"r\") as f:\n",
    "            json_file = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        missing_files_list.append(os.path.join(directory, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5323"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fakenewsnet_dataset\\\\gossipcop\\\\fake\\\\gossipcop-1000908841'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_files_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in missing_files_list:\n",
    "    os.rmdir(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_files_list = []\n",
    "\n",
    "directory = \"fakenewsnet_dataset\\gossipcop\\\\real\"\n",
    "for filename in os.listdir(directory):\n",
    "    file = os.path.join(directory, filename, \"news content.json\")\n",
    "    try:\n",
    "        with open(file, \"r\") as f:\n",
    "            json_file = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        missing_files_list.append(os.path.join(directory, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16817"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1674"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "extra_files = []\n",
    "for item in missing_files_list:\n",
    "    try:\n",
    "        shutil.rmtree(item)\n",
    "    except OSError:\n",
    "        extra_files.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make csv's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make text dataset\n",
    "Use gossipcop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "missing_files_list = []\n",
    "text = []\n",
    "label = []\n",
    "directory = \"fakenewsnet_dataset\\gossipcop\\\\real\"\n",
    "for filename in os.listdir(directory):\n",
    "    file = os.path.join(directory, filename, \"news content.json\")\n",
    "    with open(file, \"r\") as f:\n",
    "        json_file = json.load(f)\n",
    "        text.append(json_file[\"text\"])\n",
    "\n",
    "label.extend(np.ones(len(os.listdir(directory))))\n",
    "\n",
    "directory = \"fakenewsnet_dataset\\gossipcop\\\\fake\"\n",
    "for filename in os.listdir(directory):\n",
    "    file = os.path.join(directory, filename, \"news content.json\")\n",
    "    with open(file, \"r\") as f:\n",
    "        json_file = json.load(f)\n",
    "        text.append(json_file[\"text\"])\n",
    "label.extend(np.zeros(len(os.listdir(directory))))\n",
    "\n",
    "data = { \"text\" : text, \"label\" : label }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"dataset_text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make title dataset\n",
    "Use gossipcop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = split_text(df[\"text\"].values.tolist()[1], 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705\n",
      "475\n",
      "231\n"
     ]
    }
   ],
   "source": [
    "print(len(df[\"text\"].values.tolist()[1].split(\" \")))\n",
    "x = split_text(df[\"text\"].values.tolist()[1], 512)\n",
    "for i in x:\n",
    "    print(len(i.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "705"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"text\"].values.tolist()[1].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "text = []\n",
    "label = []\n",
    "directory = \"fakenewsnet_dataset\\gossipcop\\\\real\"\n",
    "for filename in os.listdir(directory):\n",
    "    file = os.path.join(directory, filename, \"news content.json\")\n",
    "    with open(file, \"r\") as f:\n",
    "        json_file = json.load(f)\n",
    "        text.append(json_file[\"title\"])\n",
    "\n",
    "label.extend(np.ones(len(os.listdir(directory))))\n",
    "\n",
    "directory = \"fakenewsnet_dataset\\gossipcop\\\\fake\"\n",
    "for filename in os.listdir(directory):\n",
    "    file = os.path.join(directory, filename, \"news content.json\")\n",
    "    with open(file, \"r\") as f:\n",
    "        json_file = json.load(f)\n",
    "        text.append(json_file[\"title\"])\n",
    "label.extend(np.zeros(len(os.listdir(directory))))\n",
    "\n",
    "data = { \"text\" : text, \"label\" : label }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"dataset_title.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105.38691784125623"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset_text_normalized-m2-128.csv\")\n",
    "np.mean([len(str(x).split(\" \")) for x in df[\"text\"].values.tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In medie sunt 573.6871098237027 intr-o propozitie nenormalizata.\n",
    "### In medie sunt 402.1050809489869 intr-o propozitie normalizata cu lungimea maxima de 512.\n",
    "### In medie sunt 105.38691784125623 intr-o propozitie normalizata cu lungimea maxima de 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.639314787993808"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset_title.csv\")\n",
    "np.mean([len(str(x).split(\" \")) for x in df[\"text\"].values.tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In medie sunt 10.639314787993808 intr-un titlu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make text normalized (to 512 words per sentence) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the moment we just clean the tabs and newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessing(text:str) -> str:\n",
    "    text = re.sub(r'[\\t\\n]', ' ', text) # remove tabs and newlines\n",
    "    text = re.sub(r'[0-9][0-9]\\.|[0-9]\\.', ' ', text) # remove lists\n",
    "    text = re.sub(r'\\[[0-9]+\\]', ' ', text) # remove citatinos - ex: [321]\n",
    "    text = re.sub(r'\\\\', ' ', text) # remove '\\' from word\\'s\n",
    "    text = re.sub(r'\\[.+\\]', ' ', text) # remove [word] - ex: [edit]\n",
    "    text = re.sub(r'\\(|\\)', ' ', text) # remove ()\n",
    "    text = re.sub(r'[=\\-\\+]+', ' ', text) # remove ()\n",
    "    text = re.sub(r' +', ' ', text) # reduce spaces to one space\n",
    "    text = re.sub(r'\\.+', '.', text) # replace ... with .\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_add_tags_text(text: str) -> list[str]:\n",
    "    news = []\n",
    "    final_article = \"[CLS]\"\n",
    "    text = preprocessing(text)\n",
    "    article = text.split(\". \")\n",
    "    for sentence in article:\n",
    "        final_article = f\"{final_article} {sentence}. [SEP]\"\n",
    "    \n",
    "    final_article = re.sub('\\.\\.', '.', final_article) # replace ... with .\n",
    "    return final_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text: str, length_max: int, length_min:int) -> list[str]:\n",
    "    sentences = []\n",
    "    num_tokens = len(text.split(\". \"))\n",
    "    text = preprocessing(text)\n",
    "    if len(text.split(\" \")) > length_max - (num_tokens + 1):\n",
    "        sentences_split = text.split(\". \")\n",
    "        total_words = 0\n",
    "        final_sentence = \"\"\n",
    "        for sentence in sentences_split:\n",
    "            words = len(sentence.split(\" \"))\n",
    "            if words + total_words > length_max - 2:\n",
    "                final_sentence = final_sentence.strip()\n",
    "                sentences.append(final_sentence)\n",
    "                total_words = 0\n",
    "                final_sentence = \"\"\n",
    "        \n",
    "            total_words += words\n",
    "            final_sentence = f\"{final_sentence}{sentence}. \"\n",
    "\n",
    "        final_sentence = final_sentence.strip()\n",
    "        if len(final_sentence.split(\" \")) > length_min:\n",
    "            final_sentence = final_sentence.strip()\n",
    "            sentences.append(final_sentence)\n",
    "    \n",
    "        sentences = [ split_add_tags_text(sentence) for sentence in sentences ]\n",
    "        return sentences\n",
    "    \n",
    "    else:\n",
    "        if len(text.split(\" \")) > length_min:\n",
    "            final_sentence = re.sub(r'\\.+', '', text) # remove ...\n",
    "            final_sentence = final_sentence.strip()\n",
    "            final_sentence = final_sentence\n",
    "            sentences.append(final_sentence)\n",
    "            \n",
    "            text = split_add_tags_text(text)\n",
    "            return [text]\n",
    "    \n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Literal\n",
    "\n",
    "def extract(directory: str, length_max:int, length_min:int, component: str, news_type: Literal[\"fake\", \"true\"]) -> Tuple[list[str], list[str]]:\n",
    "    text = []\n",
    "    label = []\n",
    "    for filename in os.listdir(directory):\n",
    "        file = os.path.join(directory, filename, \"news content.json\")\n",
    "        with open(file, \"r\") as f:\n",
    "            json_file = json.load(f)\n",
    "            news = split_text(text=json_file[component], length_max=length_max, length_min=length_min)\n",
    "            try:\n",
    "                news.remove(\"\")\n",
    "            except:\n",
    "                pass\n",
    "            text.extend(news)\n",
    "            if news_type == \"fake\":\n",
    "                label.extend(np.zeros(len(news)))\n",
    "            if news_type == \"true\":\n",
    "                label.extend(np.ones(len(news)))\n",
    "\n",
    "    return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "text = []\n",
    "label = []\n",
    "length_max = 512\n",
    "length_min = 15\n",
    "\n",
    "\n",
    "directory = \"fakenewsnet_dataset\\gossipcop\\\\real\"\n",
    "text_output, label_output = extract(directory=directory, \n",
    "                                    length_max=length_max, \n",
    "                                    length_min=length_min, \n",
    "                                    component=\"text\", \n",
    "                                    news_type=\"true\",\n",
    "                                    )\n",
    "text.extend(text_output)\n",
    "label.extend(label_output)\n",
    "\n",
    "directory = \"fakenewsnet_dataset\\gossipcop\\\\fake\"\n",
    "text_output, label_output = extract(directory=directory, \n",
    "                                    length_max=length_max, \n",
    "                                    length_min=length_min, \n",
    "                                    component=\"text\", \n",
    "                                    news_type=\"fake\",\n",
    "                                    )\n",
    "text.extend(text_output)\n",
    "label.extend(label_output)\n",
    "\n",
    "\n",
    "data = { \"text\" : text, \"label\" : label }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(f\"dataset_text_normalized-m5-M{length_max}-m{length_min}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function takes - 10s-30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "text = []\n",
    "label = []\n",
    "length_max = 128\n",
    "length_min = 5\n",
    "\n",
    "directory = \"fakenewsnet_dataset\\gossipcop\\\\real\"\n",
    "text_output, label_output = extract(directory=directory, \n",
    "                                    length_max=length_max, \n",
    "                                    length_min=length_min, \n",
    "                                    component=\"title\", \n",
    "                                    news_type=\"true\",\n",
    "                                    )\n",
    "text.extend(text_output)\n",
    "label.extend(label_output)\n",
    "\n",
    "directory = \"fakenewsnet_dataset\\gossipcop\\\\fake\"\n",
    "text_output, label_output = extract(directory=directory, \n",
    "                                    length_max=length_max, \n",
    "                                    length_min=length_min, \n",
    "                                    component=\"title\", \n",
    "                                    news_type=\"fake\",\n",
    "                                    )\n",
    "text.extend(text_output)\n",
    "label.extend(label_output)\n",
    "\n",
    "\n",
    "data = { \"text\" : text, \"label\" : label }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df.dropna(subset=[\"text\"])\n",
    "df.to_csv(\"dataset_title_normalized-M128-m5.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function takes - 7s to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"dataset_text_normalized-m3-128.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.iloc[792][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When host Andy Cohen said that she was caught in a “megawatt lie,” Parks said, “What more can I do? I apologized already and the person I’m most concerned about is Porsha. I’m sorry that it hurt Kandi as well.” A source previously told Us in late April that the reunion episodes were “disastrous” for Parks. “It would be understandable if Phaedra didn’t come back, but it’s not finalized yet,” the source said. “It wouldn’t be Phaedra’s decision, but Bravo’s.'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Still, Kesha's experience is a reminder to fans that things can always get better, even if it doesn't seem that way at first.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'\\.+', '.', text) # remove ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Britton, Swift\\'s business savvy has helped her \"excel as an authentic personality who establishes direct connections with her audience\", \"touch as many people as possible\", and \"generate a kind of advocacy and excitement that no level of advertising could\". Swift is one of the most-followed people on social media. As of April 2021, she has approximately 176 million followers on Instagram, 6 million followers on Twitter 1 million subscribers on YouTube, and is also very active on Tumblr. She joined TikTok on August 23, 2021, becoming the fastest user to amass 100,000 followers after her first upload (in 34 minutes), and surpassed 5 million followers in the first day. Swift is known for her frequent and friendly online interactions with her fans. She has visited fans in hospitals and delivered holiday gifts to them by mail and in person, an event dubbed \"Swiftmas\", and considers it her \"responsibility\" to be conscious of her influence on young fans. She has called her relationship with her fans \"the longest and best\" she has ever had. Often labeled by the media as \"America\\'s Sweetheart\", a sobriquet based on her down-to-earth personality and girl-next-door image, Swift insists she does not \"live by all these rigid, weird rules that make me feel all fenced in.'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing(text).strip() # 328"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ede5ff20b9c45f8137e95964d4b7587c122cf20b1b78154b99109cec2ed5839e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('hf': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
